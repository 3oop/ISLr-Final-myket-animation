---
title: "amar.rosa"
author: "luka"
date: "2025-02-06"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
clean_data <- read_csv("C:/Users/USER/Downloads/clean_data.csv")
df<-clean_data
```
```{r}
# Load necessary libraries
library(MASS)
library(glmnet)
library(randomForest)
library(e1071)
library(nnet)
library(mgcv)
library(xgboost)

# Remove unnecessary columns
data <- data[, -c(1, 2, 3, 7, 9)]  # Remove specified columns
X <- data[, -which(names(data) == "Amtiaz" | names(data) == "country")]
y <- data$Amtiaz

# Convert categorical variables to dummy variables
X <- model.matrix(~ . - 1, data = X)

# Split the data into training and test sets
set.seed(42)
trainIndex <- sample(1:nrow(X), 0.8 * nrow(X))
X_train <- X[trainIndex, ]
X_test <- X[-trainIndex, ]
y_train <- y[trainIndex]
y_test <- y[-trainIndex]
```

```{r}
# Load required libraries
library(xgboost)

# Convert training and test data to DMatrix format
dtrain <- xgb.DMatrix(data = X_train, label = y_train)
dtest <- xgb.DMatrix(data = X_test, label = y_test)

# Set parameters for XGBoost
params <- list(
  objective = "reg:squarederror",
  eval_metric = "rmse",
  max_depth = 6,
  eta = 0.3,
  nthread = 2,
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

# Train the XGBoost model
xgb_model <- xgb.train(params, dtrain, nrounds = 200, watchlist = list(train = dtrain), early_stopping_rounds = 10)

# Predict using the model
y_pred <- predict(xgb_model, dtest)

# Calculate Mean Squared Error
mse <- mean((y_test - y_pred)^2)
print(paste("XGBoost MSE:", mse))

# Save the model
save(xgb_model, file = "xgboost_model.RData")

```

```{r}
# Load necessary libraries
library(xgboost)
library(DALEX)
library(lime)

# Train the XGBoost model (from previous steps)
# Convert training and test data to DMatrix format
dtrain <- xgb.DMatrix(data = X_train, label = y_train)
dtest <- xgb.DMatrix(data = X_test, label = y_test)

params <- list(
  objective = "reg:squarederror",
  eval_metric = "rmse",
  max_depth = 6,
  eta = 0.3,
  nthread = 2,
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

xgb_model <- xgb.train(params, dtrain, nrounds = 200, watchlist = list(train = dtrain), early_stopping_rounds = 10)

# Global Methods
# 1. Feature Importance
importance_matrix <- xgb.importance(model = xgb_model)
print(importance_matrix)

# 2. Partial Dependence Plot (PDP)
explainer <- DALEX::explain(xgb_model, data = X_train, y = y_train, label = "XGBoost Model")
pdp <- model_profile(explainer, variable = "Total_Episodes")  # Replace "your_feature" with the feature name
plot(pdp)




```

