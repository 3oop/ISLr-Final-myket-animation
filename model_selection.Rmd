---
title: "ISL Final Project Phase III"
author: "Pooria Assarehha"
date: "`r Sys.Date()`"
output: pdf_document
editor_options: 
  markdown: 
    wrap: sentence
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Exploratory Data Analaysis

```{r}
library(ggplot2)
library(dplyr)
library(tidyr)
library(corrplot)
library(gridExtra)
library(tidyverse)
library(reshape2)
library(moments)
library(randomForest)
library(glmnet)
library(mgcv)
library(caret)
library(e1071)
library(xgboost)
library(e1071)
library(kernlab)
```

```{r}
file_path <- "C:/Users/SAM-Tech/Desktop/ISL/clean_data.csv"  
df <- read.csv(file_path, stringsAsFactors = TRUE)  
```

```{r}
summary_stats <- summary(df)
print(summary_stats)
```

Histograms for key numerical variables

```{r}
num_cols <- c("IMDB_Link", "Amtiaz", "Number_People", "Total_Episodes")

par(mfrow=c(2,2)) 
for (col in num_cols) {
  hist(df[[col]], main=paste("Distribution of", col), col="skyblue", border="black")
}
```

Correlation heatmap

```{r}
num_df <- df %>% select_if(is.numeric)
corr_matrix <- cor(num_df, use="complete.obs")

corrplot(corr_matrix, method="color", col=colorRampPalette(c("blue", "white", "red"))(200),
         tl.cex=0.35, tl.col="black", title="Correlation Heatmap")
```

Bar chart for Categorical Features, Country distribution

```{r}
ggplot(df, aes(x=Country)) +
  geom_bar(fill="skyblue", color="black") +
  theme(axis.text.x = element_text(angle=45, hjust=1)) +
  labs(title="Distribution of Movies by Country", x="Country", y="Count")
```

Bar chart for Rade distribution

```{r}
ggplot(df, aes(x=Rade)) +
  geom_bar(fill="orange", color="black") +
  theme(axis.text.x = element_text(angle=45, hjust=1)) +
  labs(title="Distribution of Movies by Rade", x="Rade", y="Count")
```

Outlier detection using boxplots

```{r}
numeric_features <- c("IMDB_Link", "Amtiaz", "Number_People", "Year", "Total_Episodes")
df_long <- df %>% select(all_of(numeric_features)) %>% pivot_longer(everything(), names_to="Feature", values_to="Value")
#df_melted <- melt(df, measure.vars = num_features)

ggplot(df_long, aes(x=Feature, y=Value)) +
  geom_boxplot(fill="lightblue", color="black") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle=45, hjust=1)) +
  labs(title="Boxplots of Key Numerical Features", x="Feature", y="Value")
```

Checking for Skewness and Distribution

```{r}
for (col in num_cols) {
  cat("Skewness of", col, ":", skewness(df[[col]], na.rm = TRUE), "\n")
}
```

Data Overview: The dataset contains 638 rows and 61 columns.
There are both numerical and categorical features.
Columns like URL, Name, and Rade are categorical, while others like IMDB_Link, Amtiaz, and Number_People are numerical.
The dataset has no missing values after preprocessing.

Numerical Features: IMDB_Link has values ranging from 2.1 to 9.3, with a mean of 6.84.
Amtiaz ranges from 17 to 100, with a mean of 84.6.
Number_People has high variance, ranging from 1,000 to 998,000.
Year values range from 1940 to 2025, with most data points concentrated in recent years.
Some numerical columns (like Total_Episodes) have skewed distributions, which might affect modeling.

Categorical Features: Country and Rade should be analyzed further with frequency counts.
Many binary genre columns (e.g., Romance, SciFi, Anime) are mostly 0s, meaning most movies don't belong to these genres.

Missing Values: No missing values are present after cleaning.

Correlation Analysis: Strong correlations exist between IMDB_Link, Amtiaz, and Number_People, indicating possible relationships worth exploring in modeling.

## Feature Selection

```{r}
data <- df

# Remove the first two columns
data <- data[, -c(1, 2)]
data <- data[, -c(1, 2)]

# Convert non-numeric columns to factors
data[] <- lapply(data, function(x) if(is.character(x)) as.factor(x) else x)

# Reduce the number of categories for large categorical predictors
data[] <- lapply(data, function(x) if(is.factor(x) && nlevels(x) > 53) {
  levels_to_keep <- names(sort(table(x), decreasing = TRUE)[1:53])
  x <- factor(ifelse(x %in% levels_to_keep, as.character(x), "Other"))
  return(x)
} else return(x))

# Define the target variable
target <- data$Amtiaz

# Remove the target variable column from the features
features <- data[, !names(data) %in% 'Amtiaz']

# Train the random forest model and calculate feature importance
rf_model <- randomForest(features, target, importance = TRUE, na.action = na.omit)

# Extract feature importance
importance <- importance(rf_model)
importance_df <- data.frame(Feature = rownames(importance), Importance = importance[, 1])

# Sort features by importance
importance_df <- importance_df[order(-importance_df$Importance), ]

# Display important features
print("Feature Importance:")
print(importance_df)

# Select the top n features (e.g., top 10 features)
top_n <- 10
top_features <- importance_df$Feature[1:top_n]

# Create a new dataset with important features
final_data <- data[, c(top_features, 'Amtiaz')]
desktop_path <- file.path(Sys.getenv("USERPROFILE"), "Desktop", "final_data")

# Save the data to a CSV file on the desktop
write_csv(final_data, desktop_path)

# Display confirmation message
print("The data has been successfully saved to a CSV file on the desktop.")

```

## Model Map

#Separating train, test and validation set
```{r}
# removing features that are unique for each record
df <- df[,-c(1,2,3)]

set.seed(1)
n <- nrow(df)
train_idx <- sample(1:n, size = 0.7 * n)  
remaining_idx <- setdiff(1:n, train_idx)  
valid_idx <- sample(remaining_idx, size = 0.15 * n)  
test_idx <- setdiff(remaining_idx, valid_idx)  
train_data <- df[train_idx, ]
valid_data <- df[valid_idx, ]
test_data  <- df[test_idx, ]
```
# simple linear regression

```{r}
linear_regression_model <- lm(Amtiaz~., data = train_data)
pred_linear_regression <- predict(linear_regression_model, valid_data)
sse_linear_regression <- sum((pred_linear_regression - valid_data$Amtiaz)^2)
sst <- sum((valid_data$Amtiaz - mean(valid_data$Amtiaz))^2)
(mse_linear_regression <- mean((pred_linear_regression - valid_data$Amtiaz)^2))
(mae_linear_regression <- mean(abs(pred_linear_regression - valid_data$Amtiaz)))
(r2_linear_regression <- 1 - sse_linear_regression/sst)
```
MSE for Linear regression :  143.9914 
MAE for Linear regression :  8.128822 
R2 for Linear regression :  -0.106155 

# ridge regression

```{r}
x <- model.matrix(Amtiaz ~ ., df)[, -1]
y <- df$Amtiaz
x_train <- x[train_idx, ]
x_test <- x[test_idx, ]
x_valid <- x[valid_idx, ]
y_train <- y[train_idx]
y_test <- y[test_idx]
y_valid <- y[valid_idx]
  
#tuning lambda parameter  
ridge_cv <- cv.glmnet(x_train, y_train, alpha = 0, lambda = 10^seq(4, -2, length = 100))
best_lambda_ridge <- ridge_cv$lambda.min
cat("Optimal Lambda for Ridge : ", best_lambda_ridge, "\n")

ridge_model <- glmnet(x_train, y_train, alpha = 0, lambda = best_lambda_ridge)
pred_ridge <- predict(ridge_model, s = best_lambda_ridge, newx = x_valid)
sse_ridge <- sum((pred_ridge - valid_data$Amtiaz)^2)
mse_ridge <- mean((pred_ridge - valid_data$Amtiaz)^2)
mae_ridge <- mean(abs(pred_ridge - valid_data$Amtiaz))
r2_ridge <- 1 - sse_ridge/sst
cat("MSE for Ridge : ", mse_ridge, "\n")
cat("MAE for Ridge : ", mae_ridge, "\n")
cat("R2 for Ridge : ", r2_ridge, "\n")
```
Optimal Lambda for Ridge :  9.326033 
MSE for Ridge :  112.9001 
MAE for Ridge :  7.251606 
R2 for Ridge :  0.1326908 

# lasso regression

```{r}
#tuning lambda parameter
lasso_cv <- cv.glmnet(x_train, y_train, alpha = 1, lambda = 10^seq(4, -2, length = 100))
best_lambda_lasso <- lasso_cv$lambda.min
cat("Optimal Lambda for Lasso : ", best_lambda_lasso, "\n")

lasso_model <- glmnet(x_train, y_train, alpha = 1, lambda = best_lambda_lasso)
pred_lasso <- predict(lasso_model, s = best_lambda_lasso, newx = x_valid)
sse_lasso <- sum((pred_lasso - valid_data$Amtiaz)^2)
mse_lasso <- mean((pred_lasso - valid_data$Amtiaz)^2)
mae_lasso <- mean(abs(pred_lasso - valid_data$Amtiaz))
r2_lasso <- 1 - sse_lasso/sst
cat("MSE for Lasso : ", mse_lasso, "\n")
cat("MAE for Lasso : ", mae_lasso, "\n")
cat("R2 for Lasso : ", r2_lasso, "\n")
```
Optimal Lambda for Lasso :  0.3764936 
MSE for Lasso :  114.0951 
MAE for Lasso :  7.167505 
R2 for Lasso :  0.1235109

#XG Boost
```{r}
#tuning parameters nrounds(number of repetitions), eta(learning rate), max_depth(trees depth), gamma(minimum reduction in waste), colsample_bytree(feature #selection percentage for each tree), min_child_weight(minimum sample weight at nodes), subsample(sampling ratio)
grid <- expand.grid(nrounds = c(50, 100, 150),   
  eta = c(0.01, 0.1, 0.3),     
  max_depth = c(3, 6, 9),
  gamma = c(0, 1, 5),          
  colsample_bytree = c(0.5, 0.7, 1), 
  min_child_weight = c(1, 3, 5),     
  subsample = c(0.6, 0.8, 1))
#10-fold cross-validation
train_control <- trainControl(method = "cv", number = 10) 
xgb_tuned <- train(x = as.matrix(x_train), y = y_train, method = "xgbTree", trControl = train_control, tuneGrid = grid)
best_params <- xgb_tuned$bestTune
cat("Optimal Parameters for XG Boost : ",  paste(best_params, collapse = ", "), "\n")

xgb_model <- xgboost(data = x_train,
  label = y_train,
  nrounds = best_params$nrounds,
  eta = best_params$eta,
  max_depth = best_params$max_depth,
  min_child_weight = best_params$min_child_weight,
  subsample = best_params$subsample,
  colsample_bytree = best_params$colsample_bytree,
  objective = "reg:squarederror")

pred_xgb <- predict(xgb_model, x_valid)
sse_xgb <- sum((pred_xgb - valid_data$Amtiaz)^2)
mse_xgb <- mean((pred_xgb - valid_data$Amtiaz)^2)
mae_xgb <- mean(abs(pred_xgb - valid_data$Amtiaz))
r2_xgb <- 1 - sse_xgb/sst
cat("MSE for XG Boost : ", mse_xgb, "\n")
cat("MAE for XG Boost : ", mae_xgb, "\n")
cat("R2 for XG Boost : ", r2_xgb, "\n")
```
Optimal Parameters for XG Boost :  50, 3, 0.1, 5, 0.5, 5, 1 
MSE for XG Boost :  111.038 
MAE for XG Boost :  7.076772 
R2 for XG Boost :  0.1469957

# SVR
```{r}
# Convert columns 
df$Country <- as.factor(df$Country)
df$Rade <- as.factor(df$Rade)
df$Amtiaz <- as.numeric(df$Amtiaz)
df$Year <- as.numeric(df$Year)
# Convert categorical variables to dummy variables
df <- dummyVars(~ ., data = df) %>% predict(df) %>% as.data.frame()

#Split the data into features and target
target <- "Amtiaz"
predictors <- setdiff(names(df), target)

# Scale numerical features
preproc <- preProcess(train_data[, predictors], method = c("center", "scale"))
train_data_scaled <- predict(preproc, train_data)
test_data_scaled <- predict(preproc, test_data)
valid_data_scaled <- predict(preproc, valid_data)
train_data_scaled$Amtiaz <- train_data$Amtiaz
test_data_scaled$Amtiaz <- test_data$Amtiaz
valid_data_scaled$Amtiaz <- valid_data$Amtiaz

#tuning parameters sigma and C(cost)
tuneGrid_svr <- expand.grid(.sigma = c(0.1, 0.5, 1), .C = c(0.1, 1, 10))
# 10-fold cross-validation
svr_tuned <- train(x_train, y_train, method = "svmRadial", tuneGrid = tuneGrid_svr, trControl = trainControl(method = "cv", number = 10))
best_params_svr <- svr_tuned$bestTune
cat("Optimal Parameters for SVR : ",  paste(best_params_svr, collapse = ", "), "\n")
svr_model <- svm(Amtiaz ~ ., data = train_data_scaled, type = "eps-regression", kernel = "radial", sigma = best_params_svr$sigma, C = best_params_svr$C)

pred_svr <- predict(svr_model,valid_data_scaled)
sse_svr <- sum((pred_svr - valid_data$Amtiaz)^2)
sst <- sum((valid_data$Amtiaz - mean(valid_data$Amtiaz))^2)
mse_svr <- mean((pred_svr - valid_data$Amtiaz)^2)
mae_svr <- mean(abs(pred_svr - valid_data$Amtiaz))
r2_svr <- 1 - sse_svr/sst
cat("MSE for SVR : ", mse_svr, "\n")
cat("MAE for SVR : ", mae_svr, "\n")
cat("R2 for SVR : ", r2_svr, "\n")

# Plot actual vs predicted values
plot(valid_data_scaled$Amtiaz, pred_svr, xlab = "Actual Amtiaz", ylab = "Predicted Amtiaz", main = "SVR Predictions vs. Actual Values", col = "green", pch = 16)
abline(0, 1, col = "red", lwd = 2)
```
Optimal Parameters for SVR :  1, 10 
MSE for SVR :  129.3761 
MAE for SVR :  7.237962 
R2 for SVR :  0.006120485 

# GAM

```{r}
data = 'clean_data.csv'
df <- read.csv(data, stringsAsFactors = TRUE)
```

```{r}
df$Country <- factor(df$Country)
df$Rade <- factor(df$Rade)
df$Country <- factor(df$Country, levels = unique(df$Country))
df$Rade <- factor(df$Rade, levels = unique(df$Rade))
df$Year <- as.numeric(df$Year)
df$Amtiaz <- as.numeric(df$Amtiaz)
df$IMDB_Link <- as.numeric(df$IMDB_Link)
## Identify binary variables and convert them to factors
#binary_vars <- setdiff(names(df), c("Country", "Rade", "Year", "Amtiaz", "IMDB_Link"))
#df[binary_vars] <- lapply(df[binary_vars], as.factor)
```

```{r}
set.seed(123)
train_index <- sample(1:nrow(df), size = floor(0.8 * nrow(df)), replace = FALSE)
train_data <- df[train_index, ]
test_data <- df[-train_index, ]

# Align test set factor levels with training set
test_data$Country <- factor(test_data$Country, levels = levels(train_data$Country))
test_data$Rade <- factor(test_data$Rade, levels = levels(train_data$Rade))
```

```{r}
gam_model <- gam(Amtiaz ~ s(Year) + s(IMDB_Link) + Country + Rade, data=train_data, method ="REML")

summary(gam_model)
```

```{r}
test_predictions <- predict(gam_model, newdata = test_data)

# Calculate test MSE and R-squared
mse_test <- mean((test_predictions - test_data$Amtiaz)^2)
rsq_test <- 1 - sum((test_predictions - test_data$Amtiaz)^2) / sum((mean(test_data$Amtiaz) - test_data$Amtiaz)^2)


train_predictions <- predict(gam_model, newdata = train_data)

# Calculate training MSE and R-squared
mse_train <- mean((train_predictions - train_data$Amtiaz)^2)
rsq_train <- 1 - sum((train_predictions - train_data$Amtiaz)^2) / sum((mean(train_data$Amtiaz) - train_data$Amtiaz)^2)

print(paste("Test Mean Squared Error:", mse_test))
print(paste("Test R-squared:", rsq_test))
print(paste("Train Mean Squared Error:", mse_train))
print(paste("Train R-squared:", rsq_train))
```

```{r}
plot(gam_model, pages = 1, shade = TRUE, col = "blue")

par(mfrow = c(2, 2))
gam.check(gam_model)
```

```{r}
ata.frame(Actual = test_data$Amtiaz, Predicted = test_predictions), 
       aes(x = Actual, y = Predicted)) +
  geom_point(color = "blue") +
  geom_abline(intercept = 0, slope = 1, col = "red", linetype = "dashed") +
  labs(title = "GAM Predictions vs. Actual Values",
       x = "Actual Amtiaz",
       y = "Predicted Amtiaz") +
  theme_minimal()
```





# random forest


```{r}
data = 'clean_data.csv'
df <- read.csv(data, stringsAsFactors = TRUE)
```

```{r}
# Convert columns 
df$Country <- as.factor(df$Country)
df$Rade <- as.factor(df$Rade)
df$Year <- as.numeric(df$Year)
df$Amtiaz <- as.numeric(df$Amtiaz)
df$IMDB_Link <- as.numeric(df$IMDB_Link)
# Convert all other binary variables to factor
#binary_vars <- setdiff(names(df), c("Country", "Rade", "Year", "Amtiaz", "IMDB_Link"))
#df[binary_vars] <- lapply(df[binary_vars], as.factor)
```

```{r}
set.seed(123)
train_index <- sample(1:nrow(df), size = floor(0.8 * nrow(df)), replace = FALSE)
train_data <- df[train_index, ]
test_data <- df[-train_index, ]
```

```{r}
target <- "Amtiaz"
predictors <- setdiff(names(df), target)
```

```{r}
rf_model <- randomForest(Amtiaz ~ ., data = train_data, ntree = 100, mtry = 3, importance = TRUE)
print(rf_model)
```

```{r}
predictions <- predict(rf_model, test_data)
head(predictions)

# For regression tasks, Mean Squared Error (MSE)
mse <- mean((predictions - test_data$Amtiaz)^2)
print(paste("Mean Squared Error:", mse))
```

```{r}
tuned_rf <- tuneRF(train_data[-which(names(train_data) == "Amtiaz")], train_data$Amtiaz, stepFactor = 1.5, improve = 0.01, ntreeTry = 100)
print(tuned_rf)
```

```{r}
importance(rf_model)
varImpPlot(rf_model)
```

```{r}
rsq <- 1 - sum((predictions - test_data$Amtiaz)^2) / sum((mean(test_data$Amtiaz) - test_data$Amtiaz)^2)
print(paste("R-squared:", rsq))
```


1)	Simple Linear Regression
This model is often the first approach to try when you believe the relationship between the variables is linear.
Pros:
Easy to implement and interpret.
Provides a clear and direct relationship between the predictor and the outcome.
Cons:
Assumes a linear relationship; might not work well if the relationship is nonlinear.
Sensitive to outliers.
the relationship between the predictors and the target is nonlinear, a more complex model is needed, like multiple regression.


2)	Ridge Regression
we have correlation among the predictors. It adds a penalty term to the least squares estimation, shrinking the coefficients to prevent overfitting. While simple linear regression is easy to implement, it doesnâ€™t handle multicollinearity well, and its performance can degrade when predictors are highly correlated.
Pros:
Reduces model complexity and prevents overfitting by regularizing the coefficients.
Useful in high-dimensional datasets.
Cons:
Does not perform feature selection (all features are included in the model).
The results might be harder to interpret due to the regularization.
We want to perform feature selection, lasso regression might be a better choice.


3)	Lasso Regression
Ridge regression shrinks coefficients but does not perform feature selection, meaning all features remain in the model, which can make interpretation difficult. Lasso regression, on the other hand, performs both regularization and feature selection by driving some coefficients to zero. This leads to a simpler, more interpretable model with fewer predictors. This is useful when we have some irrelevant predictors.
Pros:
Performs feature selection, resulting in simpler, more interpretable models.
Effective when there are many irrelevant or redundant features.
Cons:
Might eliminate useful predictors if the penalty is too high.
We use lasso regression to perform feature selection in addition to regularization.


4)	Generalized Additive Model (GAM)
This model allows for nonlinear relationships between predictors and the target variable(Amtiaz) and it fits a separate smoother for each predictor. The relationship between the predictors and Amtiaz is nonlinear but we want interpretability. While lasso regression is useful for regularization and feature selection, it assumes a linear relationship between the predictors and the target. GAM offers greater flexibility when the relationship is nonlinear and allows each predictor to have a nonlinear effect, improving model performance when there is no simple linear relationship.
Pros:
Allows for nonlinear relationships, providing flexibility in modeling complex data.
More interpretable than non-parametric models like random forests.
Cons:
Can be computationally expensive.
Requires careful tuning of smoothers.


5)	Random Forest
It is particularly useful for capturing complex, nonlinear relationships in the data. GAM is interpretable but might not perform as well as some machine learning methods when it comes to complex, high-dimensional data. We have a large number of predictors and complex interactions so that is why we use this model.
Pros:
Handles both numerical and categorical data.
Robust to overfitting and performs well even with noisy data.
Cons:
Less interpretable compared to simpler models.
Computationally expensive and can become slow with large datasets.


6)	Support Vector Regression (SVR)
t tries to find a hyperplane that best separates the data with a margin, handling nonlinear relationships using kernel functions. We have a dataset and need a powerful, nonlinear model that can handle complex relationships. Also the R-square of tree is low.
Pros:
Works well for high-dimensional spaces.
Robust to overfitting, especially in high-dimensional space.
Cons:
Sensitive to the choice of the kernel and hyperparameters.
Computationally expensive for large datasets.


7)	Neural Networks (NN)
This model is highly flexible and capable of learning complex patterns in the data through layers of interconnected nodes.
Pros:
Extremely powerful and flexible for modeling complex, nonlinear relationships.
Can learn intricate patterns from large datasets.
Cons:
Difficult to interpret due to the "black-box" nature.






